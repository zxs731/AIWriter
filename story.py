from dotenv import load_dotenv  
import os
import json, ast
import requests
import openai
import streamlit as st
import time
import re
from datetime import datetime, timezone, timedelta  

# åŠ è½½.envæ–‡ä»¶  
load_dotenv("en1106.env")  

os.environ["OPENAI_API_TYPE"] = os.environ["Azure_OPENAI_API_TYPE1"]
os.environ["OPENAI_API_BASE"] = os.environ["Azure_OPENAI_API_BASE1"]
os.environ["OPENAI_API_KEY"] = st.secrets["key"]
os.environ["OPENAI_API_VERSION"] = os.environ["Azure_OPENAI_API_VERSION1"]
BASE_URL=os.environ["OPENAI_API_BASE"]
API_KEY=os.environ["OPENAI_API_KEY"]

CHAT_DEPLOYMENT_NAME=os.environ.get('AZURE_OPENAI_API_CHAT_DEPLOYMENT_NAME')
EMBEDDING_DEPLOYMENT_NAME=os.environ.get('AZURE_OPENAI_API_EMBEDDING_DEPLOYMENT_NAME')

openai.api_type = os.environ["OPENAI_API_TYPE"]
openai.api_base = os.environ["OPENAI_API_BASE"]
openai.api_version = "2023-07-01-preview"
openai.api_key = os.getenv("OPENAI_API_KEY")

getStoryElements = {
    'name': 'getStoryElements',
    'description': 'ä¸“é—¨ç”¨äºŽæ ¹æ®æä¾›çš„å†™ä½œè¦æ±‚å’ŒèƒŒæ™¯ï¼Œç”ŸæˆåŸºæœ¬çš„æ•…äº‹å…ƒç´ ï¼Œå¦‚ï¼šæ ‡é¢˜ã€èƒŒæ™¯ã€ç›®å½•',
    'parameters': {
        'type': 'object',
        'properties': {
            'title': {
                'type': 'string',
                'description': 'æ•…äº‹çš„æ ‡é¢˜'
            },
            'background': {
                'type': 'string',
                'description': 'æ•…äº‹çš„èƒŒæ™¯'
            },
            'toc': {
                'type': 'string',
                'description': 'æ•…äº‹çš„ç« èŠ‚ç›®å½•'
            },
            'count_of_chapter': {
                'type': 'string',
                'description': 'æ•…äº‹çš„ç« èŠ‚ç›®å½•æ¡æ•°'
            }
        },
        'required': ['requirement','title','background','toc','count_of_chapter']
    }
}
tools = [
    {
        "type": "function",
        "function":getStoryElements
    }
]
def getLLMResponse(messages,tools=None,checkMinResponseLength=0):
    response = openai.ChatCompletion.create(
                engine="gpt-35-turbo-1106",
                #model="gpt-35-turbo-1106",
                messages=messages,
                tools=tools,
            )
    maxRetry=3
    curRetry=0
    while checkMinResponseLength>0 and 'tool_calls' not in response.choices[0].message and 'content' in response.choices[0].message  and len(response.choices[0].message.content)<checkMinResponseLength and curRetry<maxRetry:
        response = openai.ChatCompletion.create(
                engine="gpt-35-turbo-1106",
                #model="gpt-35-turbo-1106",
                messages=messages,
                tools=tools,
            )
        curRetry+=1
    return response
               
    

if "key" not in st.session_state:
    st.session_state.key = None

st.sidebar.markdown("# AIå†™ä½œæœºå™¨äººðŸ¤–ï¸âš¡ï¸ðŸ’Ž")
key = st.sidebar.text_input("Your key", type="password")
model=st.sidebar.selectbox(
    'Model',
    ( "gpt-35-turbo-1106", "gpt-4"))
if not key:
    st.info("Please add your key to continue.")
    st.stop()
elif str(datetime.now(timezone(timedelta(hours=8))).hour)!=key:
    st.info("Please input valid key to continue.")
    st.stop()
else:
    st.session_state.key=key    



if not key:
    st.stop()
    
uploaded_files = st.sidebar.file_uploader("Choose upload files", accept_multiple_files=True)  
current_directory = os.getcwd()  
  
for uploaded_file in uploaded_files:  
    file_path = os.path.join(current_directory, uploaded_file.name)  
    with open(file_path, 'wb') as f:  
        f.write(uploaded_file.read())  
    st.write(f'Received your file {uploaded_file.name}!')  

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    print(message)
    if "role" in message and (message["role"]=="user" or message["role"]=="assistant" ) and message["content"] is not None:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
                   


def writeReply(cont,msg):
    #print(msg)
    cont.write(msg)
    print(msg)
    

if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown('Please waiting...')
        exp = st.expander("AI messages trace", expanded=False)
        requirement=prompt
        sys_msg={"role":"system","content":"ä½ æ˜¯ä¸€ä¸ªè™šæ‹Ÿå†™ä½œåŠ©æ‰‹ã€‚"}
        user_msg={"role":"user","content":f'æ ¹æ®å†™ä½œè¦æ±‚ï¼š{requirement}ï¼Œå†™ä¸€ä¸ªè™šæ‹Ÿå°è¯´çš„æ ‡é¢˜ï¼ŒèƒŒæ™¯ï¼Œç« èŠ‚ç­‰ã€‚'}
        res=getLLMResponse([sys_msg,user_msg],tools=tools)
        print("elements creation done!")
        
        print(res.choices[0].message.tool_calls[0].function.arguments)
        j=json.loads(res.choices[0].message.tool_calls[0].function.arguments)
        assistant_response= f'''
å†™ä½œè¦æ±‚:{requirement}\n
æ ‡é¢˜: {j["title"]}\n
èƒŒæ™¯: {j["background"]}\n
ç« èŠ‚:\n{j["toc"]}\n'''
        full_response=''
        for chunk in re.split(r'(\s+)', assistant_response):
            full_response += chunk + " "
            time.sleep(0.01)

            # Add a blinking cursor to simulate typing
            message_placeholder.markdown(full_response + "â–Œ")
            
        pre_msg=None
        for i in range(1, int(j["count_of_chapter"])+1):  
            user_msg={"role":"user","content":f'''æŒ‰ç…§å¦‚ä¸‹ä¿¡æ¯å†™ä½œæŒ‡å®šç« èŠ‚ã€‚
æ ‡é¢˜:{j["title"]}
èƒŒæ™¯:{j["background"]}
ç« èŠ‚åˆ—è¡¨:{j["toc"]}
é€‰å®šçš„ç« èŠ‚:ç¬¬{i}ç« 
æŒ‰ç…§ä¸Šè¿°è¦æ±‚ç¼–å†™é€‰å®šç« èŠ‚å†…å®¹ã€‚'''}
            if pre_msg is not None:
                res=getLLMResponse([sys_msg]+pre_msg+[user_msg],tools=None,checkMinResponseLength=100) 
            else:
                res=getLLMResponse([sys_msg,user_msg],tools=None,checkMinResponseLength=100) 
            print(res.choices[0].message.content)
            exp.write(pre_msg)
            assistant_response = f'\n\n{res.choices[0].message.content}'
            pre_msg=[user_msg,{"role":"assistant","content":assistant_response}]
            
            for chunk in re.split(r'(\s+)', assistant_response):
                full_response += chunk + " "
                time.sleep(0.01)
    
                # Add a blinking cursor to simulate typing
                message_placeholder.markdown(full_response + "â–Œ")
        
        message_placeholder.markdown(full_response)
        st.session_state.messages.append({"role": "assistant", "content":full_response})
        #st.session_state.messages.append({"role": "assistant", "content": re})
